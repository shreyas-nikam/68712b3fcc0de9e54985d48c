id: 68712b3fcc0de9e54985d48c_user_guide
summary: User Guide for 68712b3fcc0de9e54985d48c
feedback link: https://docs.google.com/forms/d/e/1FAIpQLSfWkOK-in_bMMoHSZfcIvAeO58PAH9wrDqcxnJABHaxiDqhSA/viewform?usp=sf_link
environments: Web
status: Published
\n\n\n# QuLab: Operational Risk Modeling with Streamlit\n\n## 1. Introduction to QuLab and Operational Risk Modeling\nDuration: 00:05:00\n\nWelcome to **QuLab**, an interactive Streamlit application designed to demystify complex concepts in operational risk modeling. This codelab will guide you through the application's core functionalities, providing insights into the theoretical underpinnings and practical implementations. Operational risk, often defined as the risk of loss resulting from inadequate or failed internal processes, people, and systems or from external events, is a critical component of financial risk management.\n\n<aside class=\"positive\">\nThis codelab will bridge the gap between abstract theoretical concepts and their practical application using a hands-on approach with a Streamlit-powered tool. Understanding these concepts is **crucial** for quantitative analysts, risk managers, and developers working in the financial sector.\n</aside>\n\n**Key Concepts Explored:**\n*   **Scenario Analysis:** Quantifying expert judgment on potential future operational losses.\n*   **Internal Loss Data (ILD):** The empirical foundation of operational risk, derived from historical loss events.\n*   **Distribution Fitting:** Applying statistical methods to model the frequency and severity of operational losses.\n*   **Distribution Combination:** Aggregating risks from different sources (ILD and scenarios).\n*   **The Stability Paradox:** A counter-intuitive phenomenon in risk aggregation where seemingly positive changes in the body of a distribution can lead to an increase in overall tail risk.\n\n**Application Architecture:**\n\nQuLab is built using Streamlit, a powerful Python library for creating interactive web applications. The application's structure is modular, with the main `app.py` script serving as the entry point and navigation hub, dispatching control to dedicated page scripts located in the `application_pages` directory.\n\n```mermaid\ngraph TD\n    A[app.py] --> B(Sidebar Navigation)\n    B --> C{Page Selection}\n    C -- \"Scenario Fitting\" --> D[page1.py: Scenario Distribution Fitting]\n    C -- \"ILD Generation and Fitting\" --> E[page2.py: Synthetic ILD Generation and Fitting]\n    C -- \"Distribution Combination and Paradox Simulation\" --> F[page3.py: Distribution Combination and Stability Paradox Simulation]\n```\nThis modular design enhances maintainability and allows for clear separation of concerns, making it easier to develop and understand each functionality independently.\n\n## 2. Setting Up Your Development Environment\nDuration: 00:10:00\n\nTo get started with QuLab, you'll need to set up your Python development environment.\n\n### Prerequisites\n\nEnsure you have Python 3.8+ installed on your system. You can download it from the [official Python website](https://www.python.org/downloads/).\n\n### Step 1: Create a Virtual Environment\n\nIt's a best practice to use a virtual environment to manage dependencies for your project. This prevents conflicts with other Python projects on your machine.\n\nOpen your terminal or command prompt and navigate to your desired project directory. Then, run the following commands:\n\n```bash\npython -m venv quolab_env\n```\nThis command creates a new virtual environment named `quolab_env` in your current directory.\n\n### Step 2: Activate the Virtual Environment\n\nBefore installing libraries, you need to activate your virtual environment.\n\nOn macOS/Linux:\n```bash\nsource quolab_env/bin/activate\n```\nOn Windows:\n```bash\n.\\quolab_env\\Scripts\\activate\n```\nYou should see `(quolab_env)` at the beginning of your terminal prompt, indicating that the virtual environment is active.\n\n### Step 3: Install Required Libraries\n\nWith the virtual environment active, install all necessary libraries using `pip`.\n\n<button>\n  [Download requirements.txt](https://raw.githubusercontent.com/streamlit/streamlit-example-app/master/requirements.txt) (This is a placeholder, as the actual `requirements.txt` would need to be generated from the provided code. The required libraries are: `streamlit`, `scipy`, `numpy`, `pandas`, `plotly`.)\n</button>\n\nFor QuLab, the core libraries are:\n```console\nstreamlit\nscipy\nnumpy\npandas\nplotly\n```\nYou can install them all at once:\n```bash\npip install streamlit scipy numpy pandas plotly\n```\n\n### Step 4: Save the Application Code\n\nCreate the following file structure and save the respective Python code into each file:\n\n```\n├── app.py\n└── application_pages/\n    ├── __init__.py  (empty file)\n    ├── page1.py\n    ├── page2.py\n    └── page3.py\n```\n\n**app.py**\n```python\nimport streamlit as st\nst.set_page_config(page_title=\"QuLab\", layout=\"wide\")\nst.sidebar.image(\"https://www.quantuniversity.com/assets/img/logo5.jpg\")\nst.sidebar.divider()\nst.title(\"QuLab\")\nst.divider()\nst.markdown(\"\"\"\nIn this lab, we explore operational risk modeling concepts, bridging the gap between theoretical understanding and practical application.\nWe'll define hypothetical scenarios, simulate Internal Loss Data (ILD), and combine these data sources using statistical averaging techniques.\nA key objective is to visually demonstrate the counter-intuitive 'stability paradox' in risk aggregation.\n\"\"\")\n# Your code starts here\npage = st.sidebar.selectbox(label=\"Navigation\", options=[\"Scenario Fitting\", \"ILD Generation and Fitting\", \"Distribution Combination and Paradox Simulation\"])\nif page == \"Scenario Fitting\":\n    from application_pages.page1 import run_page1\n    run_page1()\nelif page == \"ILD Generation and Fitting\":\n    from application_pages.page2 import run_page2\n    run_page2()\nelif page == \"Distribution Combination and Paradox Simulation\":\n    from application_pages.page3 import run_page3\n    run_page3()\n# Your code ends\n```\n\n**application_pages/page1.py**\n```python\nimport streamlit as st\nimport scipy.stats as stats\nimport numpy as np\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\ndef fit_scenario_distribution(frequency, percentile_50, percentile_90, percentile_99):\n    \"\"\"Fits a distribution to scenario data.\n\n    Args:\n        frequency: Expected annual frequency.\n        percentile_50: 50th percentile loss.\n        percentile_90: 90th percentile loss.\n        percentile_99: 99th percentile loss.\n\n    Returns:\n        Fitted distribution object and its parameters.\n    \"\"\"\n    # Using lognorm as an example distribution\n    # Convert percentiles to log scale\n    # This is a simplified fitting process for demonstration.\n    # A more robust fitting would involve more data points or optimization.\n\n    # Approximating parameters based on percentiles for lognorm\n    # log(median) = mu\n    # log(90th percentile) = mu + sigma * inv_cdf(0.9)\n    # log(99th percentile) = mu + sigma * inv_cdf(0.99)\n    # Using 50th and 90th percentile to estimate mu and sigma roughly\n\n    try:\n        # Using a two-point estimation for lognorm parameters (simplified)\n        # For lognormal, P(X <= x) = P(ln(X) <= ln(x))\n        # So, ln(x) is normally distributed with mean=mu_log, std=sigma_log\n        # We have ln(x_p) = mu_log + sigma_log * norm.ppf(p)\n\n        # From 50th percentile: ln(percentile_50) = mu_log + sigma_log * norm.ppf(0.5) = mu_log\n        mu_log = np.log(percentile_50)\n\n        # From 90th percentile: ln(percentile_90) = mu_log + sigma_log * norm.ppf(0.9)\n        # sigma_log = (ln(percentile_90) - mu_log) / norm.ppf(0.9)\n        # Ensure percentile_90 > percentile_50\n        if percentile_90 <= percentile_50:\n            st.error(\"90th Percentile must be greater than 50th Percentile.\")\n            return None, None\n        sigma_log = (np.log(percentile_90) - mu_log) / stats.norm.ppf(0.9)\n\n        if sigma_log <= 0:\n            st.error(\"Calculated sigma_log is not positive. Please check percentile values.\")\n            return None, None\n\n        # The parameters for scipy.stats.lognorm are s (shape), loc (location), scale\n        # s = sigma_log\n        # scale = exp(mu_log) (this is the geometric mean)\n        # loc = 0 (standard assumption for loss distributions)\n\n        s_param = sigma_log\n        loc_param = 0 # Assuming losses are positive\n        scale_param = np.exp(mu_log)\n\n        # Validate with 99th percentile (optional, for consistency check)\n        # expected_99th_log = mu_log + sigma_log * stats.norm.ppf(0.99)\n        # expected_99th = np.exp(expected_99th_log)\n        # st.write(f\"Consistency check: Expected 99th percentile based on 50th/90th fit: {expected_99th:,.2f}\")\n\n        return stats.lognorm(s=s_param, loc=loc_param, scale=scale_param), (s_param, loc_param, scale_param)\n    except Exception as e:\n        st.error(f\"Error fitting distribution: {e}. Please ensure percentiles are valid and increasing.\")\n        return None, None\n\n\ndef run_page1():\n    st.header(\"Scenario Distribution Fitting\")\n    st.markdown(\"\"\"\n    This section allows you to define hypothetical operational risk scenarios by specifying\n    their expected frequency and various loss percentiles. The application will then fit a\n    statistical distribution (Log-Normal in this case) to these expert assessments,\n    providing a quantitative representation of the scenario's potential impact.\n\n    This process is crucial for converting qualitative risk insights into a quantifiable form\n    that can be used in aggregated risk models.\n    \"\"\")\n\n    st.subheader(\"Define Scenario Parameters\")\n    col1, col2 = st.columns(2)\n    with col1:\n        frequency = st.number_input(\n            \"Expected Annual Frequency (Scenario)\",\n            value=1.0,\n            min_value=0.01,\n            help=\"Expected number of loss events per year for the scenario.\"\n        )\n    with col2:\n        percentile_50 = st.number_input(\n            \"50th Percentile Loss (Scenario)\",\n            value=1_000_000.0,\n            min_value=1.0,\n            help=\"The loss value below which 50\\% of scenario losses fall.\"\n        )\n        percentile_90 = st.number_input(\n            \"90th Percentile Loss (Scenario)\",\n            value=5_000_000.0,\n            min_value=1.0,\n            help=\"The loss value below which 90\\% of scenario losses fall. Must be greater than 50th percentile.\"\n        )\n        percentile_99 = st.number_input(\n            \"99th Percentile Loss (Scenario)\",\n            value=10_000_000.0,\n            min_value=1.0,\n            help=\"The loss value below which 99\\% of scenario losses fall. Must be greater than 90th percentile.\"\n        )\n\n    if not (percentile_90 > percentile_50 and percentile_99 > percentile_90):\n        st.warning(\"Please ensure percentiles are strictly increasing: 50th < 90th < 99th.\")\n    else:\n        st.subheader(\"Fitted Scenario Distribution\")\n        fitted_dist, params = fit_scenario_distribution(frequency, percentile_50, percentile_90, percentile_99)\n\n        if fitted_dist and params:\n            st.write(f\"**Fitted Distribution Type:** Log-Normal\")\n            st.write(f\"**Estimated Parameters (s, loc, scale):** s={params[0]:.4f}, loc={params[1]:.4f}, scale={params[2]:,.2f}\")\n\n            st.markdown(\"\"\"\n            The Log-Normal distribution is characterized by its shape parameter $s$ (sigma of the underlying normal distribution),\n            location parameter $\\text{loc}$ (typically 0 for loss distributions), and scale parameter $\\text{scale}$\n            (geometric mean, $e^{\\text{mu}}$ of the underlying normal distribution).\n            \"\"\")\n            st.latex(r\"f(x; s, \\text{loc}, \\text{scale}) = \\frac{1}{x s \\sqrt{2\\pi}} \\exp\\left(-\\frac{(\\ln(x - \\text{loc}) - \\ln(\\text{scale}))^2}{2s^2}\\right)\")\n            st.latex(r\"\\text{where } x > \\text{loc}\")\n            st.latex(r\"\\text{s = sigma, scale = } e^{\\text{mu}}\")\n\n            # Plot PDF and CDF\n            x = np.linspace(max(0.1, fitted_dist.ppf(0.001)), fitted_dist.ppf(0.999), 500)\n            pdf = fitted_dist.pdf(x)\n            cdf = fitted_dist.cdf(x)\n\n            fig = make_subplots(rows=1, cols=2, subplot_titles=('Probability Density Function (PDF)', 'Cumulative Distribution Function (CDF)'))\n\n            fig.add_trace(go.Scatter(x=x, y=pdf, mode='lines', name='PDF', line=dict(color='blue')), row=1, col=1)\n            fig.update_xaxes(title_text=\"Loss Amount\", row=1, col=1)\n            fig.update_yaxes(title_text=\"Density\", row=1, col=1)\n\n            fig.add_trace(go.Scatter(x=x, y=cdf, mode='lines', name='CDF', line=dict(color='red')), row=1, col=2)\n            fig.update_xaxes(title_text=\"Loss Amount\", row=1, col=2)\n            fig.update_yaxes(title_text=\"Probability\", row=1, col=2)\n\n            fig.update_layout(height=400, showlegend=False, title_text=\"Fitted Scenario Distribution PDF and CDF\",\n                              font=dict(size=12))\n            st.plotly_chart(fig, use_container_width=True)\n\n            st.markdown(\"\"\"\n            **Interpretation of Plots:**\n            *   **PDF (Probability Density Function):** Shows the relative likelihood of a loss occurring at a given amount. The peak indicates the most probable loss range.\n            *   **CDF (Cumulative Distribution Function):** Shows the probability that a loss will be less than or equal to a given amount. For example, the point where the CDF reaches 0.90 corresponds to the 90th percentile loss.\n            \"\"\")\n\n        else:\n            st.error(\"Could not fit scenario distribution with the provided parameters. Please adjust values.\")\n```\n\n**application_pages/page2.py**\n```python\nimport streamlit as st\nimport scipy.stats as stats\nimport numpy as np\nimport pandas as pd\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom scipy.stats import poisson, lognorm, pareto, genpareto\nfrom plotly.subplots import make_subplots\n\n@st.cache_data\ndef generate_synthetic_ild(frequency_params, severity_params, num_observations, reporting_threshold):\n    \"\"\"Generates synthetic operational loss data.\"\"\"\n    if not isinstance(frequency_params, dict):\n        raise TypeError(\"frequency_params must be a dictionary\")\n    if not isinstance(severity_params, dict):\n        raise TypeError(\"severity_params must be a dictionary\")\n\n    losses = []\n    for i in range(num_observations):\n        # Generate number of losses for the year\n        if frequency_params['distribution'] == 'poisson':\n            num_losses = poisson.rvs(frequency_params['lambda'])\n        else:\n            raise ValueError(\"Invalid frequency distribution\")\n\n        # Generate severity amounts for each loss\n        severity_amounts = []\n        if severity_params['distribution'] == 'lognorm':\n            shape = severity_params['sigma']\n            loc = 0\n            scale = np.exp(severity_params['mean']) # This assumes mean is geometric mean\n            # If 'mean' is arithmetic mean, a more complex transformation is needed\n            # For simplicity, let's assume 'mean' corresponds to the scale parameter (exp(mu))\n            severity_amounts = lognorm.rvs(s=shape, loc=loc, scale=scale, size=num_losses)\n        elif severity_params['distribution'] == 'pareto':\n             b = severity_params['shape']\n             # Ensure scale is positive\n             scale_param = max(1e-9, severity_params['scale'])\n             severity_amounts = pareto.rvs(b, loc=severity_params['loc'], scale=scale_param, size=num_losses)\n        elif severity_params['distribution'] == 'gpd':\n             c = severity_params['shape']\n             # Ensure scale is positive\n             scale_param = max(1e-9, severity_params['scale'])\n             severity_amounts = genpareto.rvs(c, loc=severity_params['loc'], scale=scale_param, size=num_losses)\n        else:\n            raise ValueError(\"Invalid severity distribution\")\n\n        losses.extend(severity_amounts)\n\n    # Filter losses based on reporting threshold\n    losses = [loss for loss in losses if loss >= reporting_threshold]\n\n    # Create DataFrame\n    if losses:\n        df = pd.DataFrame({'Amount': losses})\n        df['Loss_ID'] = range(1, len(df) + 1)\n        df = df[['Loss_ID', 'Amount']]\n    else:\n        df = pd.DataFrame(columns=['Loss_ID', 'Amount'])\n\n    return df\n\n@st.cache_data\ndef fit_ild_distribution(ild_data, distribution_type, threshold):\n    \"\"\"Fits a distribution to ILD data.\"\"\"\n\n    if ild_data.empty:\n        st.warning(\"ILD data is empty. Cannot fit distribution.\")\n        return None, None\n\n    if distribution_type == \"lognorm\":\n        # Fit Log-Normal distribution\n        # Ensure data is positive for lognorm fit\n        positive_ild_data = ild_data[ild_data['Amount'] > 0]['Amount']\n        if positive_ild_data.empty:\n            st.warning(\"No positive loss amounts to fit Log-Normal distribution.\")\n            return None, None\n        shape, loc, scale = stats.lognorm.fit(positive_ild_data, floc=0) # floc=0 fixes location at 0\n        distribution = stats.lognorm\n        params = (shape, loc, scale)\n        return distribution, params\n    elif distribution_type == \"bodytail\":\n        # Fit body-tail distribution (empirical body, GPD tail)\n        if threshold is None:\n            raise ValueError(\"Threshold must be specified for body-tail distribution.\")\n        \n        # Ensure threshold is within data range for a meaningful split\n        if threshold >= ild_data['Amount'].max() and len(ild_data['Amount']) > 0:\n            st.warning(\"Threshold is too high: All data points are below or equal to the threshold. Adjust threshold to be lower than the maximum loss amount for a meaningful tail.\")\n            return None, None\n        if threshold <= ild_data['Amount'].min() and len(ild_data['Amount']) > 0:\n            st.warning(\"Threshold is too low: All data points are above or equal to the threshold. Adjust threshold to be higher than the minimum loss amount for a meaningful body/tail split.\")\n            return None, None\n\n        body_data = ild_data[ild_data['Amount'] <= threshold]['Amount']\n        tail_data = ild_data[ild_data['Amount'] > threshold]['Amount']\n\n        if len(tail_data) < 5: # Need a reasonable number of points for GPD fit\n             st.warning(f\"Not enough data points ({len(tail_data)}) in the tail for a robust GPD fit. Consider lowering the threshold or generating more ILD.\")\n             return None, None\n\n        # Fit GPD to the tail\n        # GPD parameters are c (shape), loc (location), scale\n        try:\n            # For GPD, location should typically be the threshold itself or 0 depending on definition\n            # stats.genpareto.fit takes data, and optionally floc (fixed location) or initial guess\n            gpd_params = stats.genpareto.fit(tail_data, floc=threshold)\n            gpd_dist = stats.genpareto(*gpd_params)\n        except Exception as e:\n            st.error(f\"Error fitting GPD to tail data: {e}. Check threshold and data distribution.\")\n            return None, None\n\n        # For the body, we can represent it empirically with a histogram\n        # Create bins for the body data\n        if not body_data.empty:\n            hist, bin_edges = np.histogram(body_data, bins='auto', density=True)\n            body_dist = stats.rv_histogram((hist, bin_edges))\n        else:\n            body_dist = None # No body data to fit\n\n        # Returning distributions and parameters for both body and tail\n        # Note: the `distribution` here is a tuple: (body_distribution_object, tail_distribution_object)\n        # params here is the gpd_params for the tail\n        return (body_dist, gpd_dist), gpd_params\n    else:\n        raise ValueError(\"Invalid distribution type. Choose 'lognorm' or 'bodytail'.\")\n\n\ndef run_page2():\n    st.header(\"Synthetic ILD Generation and Fitting\")\n    st.markdown(\"\"\"\n    This section simulates historical operational losses (Internal Loss Data - ILD) and allows\n    you to fit statistical distributions to this generated data. This is crucial for\n    understanding the empirical characteristics of your loss experience and for\n    extrapolating to potential future losses.\n    \"\"\")\n\n    st.subheader(\"1. Generate Synthetic ILD\")\n    st.markdown(\"\"\"\n    Configure the parameters below to generate a synthetic dataset of operational losses.\n    You can specify the frequency and severity distribution characteristics, along with\n    the number of simulation periods and a reporting threshold (minimum loss amount recorded).\n    \"\"\")\n\n    # ILD Generation Inputs\n    col1, col2 = st.columns(2)\n    with col1:\n        st.markdown(\"**Frequency Parameters**\")\n        frequency_dist_ild = st.selectbox(\n            \"ILD Frequency Distribution\",\n            [\"poisson\"],\n            help=\"Choose the distribution for the number of losses per period.\"\n        )\n        frequency_lambda_ild = st.number_input(\n            \"ILD Frequency Rate (lambda)\",\n            value=100,\n            min_value=1,\n            help=\"Average number of loss events per period (for Poisson distribution).\"\n        )\n        num_observations = st.slider(\n            \"Number of Simulation Periods (ILD)\",\n            value=5,\n            min_value=1,\n            max_value=20,\n            help=\"Number of periods (e.g., years) to simulate ILD for.\"\n        )\n    with col2:\n        st.markdown(\"**Severity Parameters**\")\n        severity_dist_ild = st.selectbox(\n            \"ILD Severity Distribution\",\n            [\"lognorm\", \"pareto\", \"gpd\"],\n            help=\"Choose the distribution for individual loss amounts.\"\n        )\n        severity_params_ild = {}\n        if severity_dist_ild == \"lognorm\":\n            severity_params_ild[\"mean\"] = st.number_input(\"Mean (lognorm)\", value=10000.0, help=\"Geometric mean for log-normal severity.\")\n            severity_params_ild[\"sigma\"] = st.number_input(\"Sigma (lognorm)\", value=0.5, min_value=0.01, help=\"Standard deviation of log-transformed data for log-normal severity.\")\n        elif severity_dist_ild == \"pareto\":\n            severity_params_ild[\"shape\"] = st.number_input(\"Shape (Pareto)\", value=1.0, min_value=0.01, help=\"Shape parameter 'b' for Pareto severity.\")\n            severity_params_ild[\"loc\"] = st.number_input(\"Location (Pareto)\", value=0.0, help=\"Location parameter for Pareto severity.\")\n            severity_params_ild[\"scale\"] = st.number_input(\"Scale (Pareto)\", value=1000.0, min_value=0.01, help=\"Scale parameter for Pareto severity.\")\n        elif severity_dist_ild == \"gpd\":\n            severity_params_ild[\"shape\"] = st.number_input(\"Shape (GPD)\", value=0.1, help=\"Shape parameter 'c' for GPD severity.\")\n            severity_params_ild[\"loc\"] = st.number_input(\"Location (GPD)\", value=0.0, help=\"Location parameter for GPD severity.\")\n            severity_params_ild[\"scale\"] = st.number_input(\"Scale (GPD)\", value=1000.0, min_value=0.01, help=\"Scale parameter for GPD severity.\")\n\n        reporting_threshold = st.number_input(\n            \"ILD Reporting Threshold\",\n            value=1000.0,\n            min_value=0.0,\n            help=\"Only losses above this amount are recorded in ILD.\"\n        )\n\n    frequency_params_ild = {\"distribution\": frequency_dist_ild, \"lambda\": frequency_lambda_ild}\n    severity_params_ild[\"distribution\"] = severity_dist_ild # Add distribution type to severity params dict\n\n    ild_data = generate_synthetic_ild(frequency_params_ild, severity_params_ild, num_observations, reporting_threshold)\n\n    if not ild_data.empty:\n        st.subheader(\"Generated ILD Data\")\n        st.write(f\"Total {len(ild_data)} losses generated above the reporting threshold of {reporting_threshold:,.2f}.\")\n        st.dataframe(ild_data.head()) # Display first few rows\n        st.write(f\"**Summary Statistics:**\")\n        st.write(f\"- Total Losses: ${ild_data['Amount'].sum():,.2f}\")\n        st.write(f\"- Mean Loss Amount: ${ild_data['Amount'].mean():,.2f}\")\n        st.write(f\"- Max Loss Amount: ${ild_data['Amount'].max():,.2f}\")\n\n        fig_hist = px.histogram(ild_data, x=\"Amount\", nbins=50, title=\"Histogram of Generated ILD\",\n                                labels={'Amount': 'Loss Amount'}, height=400)\n        fig_hist.update_layout(font=dict(size=12))\n        st.plotly_chart(fig_hist, use_container_width=True)\n    else:\n        st.warning(\"No ILD data generated. Please check your frequency, severity parameters, and reporting threshold. \"\n                   \"It's possible no losses were generated above the threshold.\")\n\n    st.subheader(\"2. Fit Distribution to ILD\")\n    st.markdown(\"\"\"\n    Now, fit a statistical distribution to the generated ILD. You can choose between a single\n    Log-Normal distribution or a Body-Tail approach, where the body of the distribution\n    is empirical and the tail is modeled by a Generalized Pareto Distribution (GPD).\n    \"\"\")\n\n    ild_fit_type = st.selectbox(\n        \"ILD Distribution Fit Type\",\n        [\"lognorm\", \"bodytail\"],\n        help=\"Choose 'lognorm' for a single distribution fit, or 'bodytail' for a combined empirical/GPD fit.\"\n    )\n    threshold_bodytail = None\n    if ild_fit_type == \"bodytail\":\n        threshold_bodytail = st.number_input(\n            \"Threshold for Body-Tail Fit\",\n            value=ild_data['Amount'].quantile(0.75) if not ild_data.empty else 10000.0,\n            min_value=0.0,\n            help=\"Losses above this threshold are modeled by GPD (for 'bodytail' fit type).\"\n        )\n\n    if not ild_data.empty:\n        fitted_ild_dist, ild_fit_params = fit_ild_distribution(ild_data, ild_fit_type, threshold_bodytail)\n\n        if fitted_ild_dist and ild_fit_params:\n            st.write(f\"**Fitted ILD Distribution Type:** {ild_fit_type.replace('lognorm', 'Log-Normal').replace('bodytail', 'Body-Tail (Empirical Body / GPD Tail)')}\")\n\n            if ild_fit_type == \"lognorm\":\n                st.write(f\"**Estimated Parameters (s, loc, scale):** s={ild_fit_params[0]:.4f}, loc={ild_fit_params[1]:.4f}, scale={ild_fit_params[2]:,.2f}\")\n                dist_to_plot = fitted_ild_dist\n                st.markdown(\"\"\"\n                For Log-Normal fit, the parameters are $s$ (shape), $\\text{loc}$ (location), and $\\text{scale}$.\n                \"\"\")\n\n                x_min_plot = max(0.1, dist_to_plot.ppf(0.001))\n                x_max_plot = dist_to_plot.ppf(0.999) if dist_to_plot.ppf(0.999) < ild_data['Amount'].max() * 2 else ild_data['Amount'].max() * 2 # avoid extremely large x-ranges\n                x_plot = np.linspace(x_min_plot, x_max_plot, 500)\n                pdf_plot = dist_to_plot.pdf(x_plot)\n                cdf_plot = dist_to_plot.cdf(x_plot)\n\n                fig_fit = make_subplots(rows=1, cols=2, subplot_titles=('Fitted PDF vs. ILD Histogram', 'Fitted CDF'))\n                \n                # PDF subplot\n                fig_fit.add_trace(go.Histogram(x=ild_data['Amount'], histnorm='probability density', name='ILD Histogram', opacity=0.6), row=1, col=1)\n                fig_fit.add_trace(go.Scatter(x=x_plot, y=pdf_plot, mode='lines', name='Fitted PDF', line=dict(color='blue', width=2)), row=1, col=1)\n                fig_fit.update_xaxes(title_text=\"Loss Amount\", row=1, col=1)\n                fig_fit.update_yaxes(title_text=\"Density\", row=1, col=1)\n                \n                # CDF subplot\n                fig_fit.add_trace(go.Scatter(x=x_plot, y=cdf_plot, mode='lines', name='Fitted CDF', line=dict(color='red', width=2)), row=1, col=2)\n                \n                # Add empirical CDF of ILD data for comparison\n                sorted_ild = np.sort(ild_data['Amount'])\n                y_ecdf = np.arange(1, len(sorted_ild) + 1) / len(sorted_ild)\n                fig_fit.add_trace(go.Scatter(x=sorted_ild, y=y_ecdf, mode='lines', name='ILD ECDF', line=dict(color='orange', dash='dash'), showlegend=True), row=1, col=2)\n\n                fig_fit.update_xaxes(title_text=\"Loss Amount\", row=1, col=2)\n                fig_fit.update_yaxes(title_text=\"Probability\", row=1, col=2)\n\n                fig_fit.update_layout(height=450, showlegend=True, title_text=\"Fitted ILD Distribution (Log-Normal)\",\n                                      font=dict(size=12), legend=dict(x=0.01, y=0.99))\n                st.plotly_chart(fig_fit, use_container_width=True)\n\n\n            elif ild_fit_type == \"bodytail\":\n                body_dist, gpd_dist = fitted_ild_dist\n                gpd_params = ild_fit_params # shape, loc, scale for GPD\n                st.write(f\"**GPD Tail Parameters (c, loc, scale):** c={gpd_params[0]:.4f}, loc={gpd_params[1]:.4f}, scale={gpd_params[2]:,.2f}\")\n                st.markdown(\"\"\"\n                For the Body-Tail fit:\n                *   The **body** of the distribution (losses $\\leq$ threshold) is represented empirically (histogram).\n                *   The **tail** of the distribution (losses $>$ threshold) is modeled by a Generalized Pareto Distribution (GPD).\n                The GPD is characterized by its shape parameter $\\xi$ ($c$), scale parameter $\\sigma$ (scale), and location parameter $u$ (loc).\n                \"\"\")\n                st.latex(r\"G_{\\xi,\\sigma}(x) = 1 - \\left(1 + \\frac{\\xi (x-u)}{\\sigma}\\right)^{-1/\\xi}\")\n                st.latex(r\"\\text{for } x > u\")\n                st.latex(r\"\\text{If } \\xi = 0, G_{0,\\sigma}(x) = 1 - e^{-(x-u)/\\sigma}\")\n\n\n                # Plotting body and tail\n                fig_bodytail = go.Figure()\n\n                # Histogram of all ILD data for context\n                fig_bodytail.add_trace(go.Histogram(x=ild_data['Amount'], histnorm='probability density', name='ILD Histogram (All)', opacity=0.5, marker_color='grey'))\n\n                # Plot fitted GPD for tail\n                if threshold_bodytail is not None and gpd_dist:\n                    tail_data_max = ild_data['Amount'].max()\n                    x_tail = np.linspace(threshold_bodytail, tail_data_max * 1.5, 200) # Extend beyond max data for visualization\n                    # Filter for x > threshold to ensure GPD is plotted for excesses\n                    x_tail = x_tail[x_tail > threshold_bodytail]\n                    if len(x_tail) > 0:\n                        pdf_gpd = gpd_dist.pdf(x_tail)\n                        fig_bodytail.add_trace(go.Scatter(x=x_tail, y=pdf_gpd, mode='lines', name='Fitted GPD (Tail)', line=dict(color='purple', width=2)))\n                        # Add a vertical line for the threshold\n                        fig_bodytail.add_vline(x=threshold_bodytail, line_dash=\"dash\", line_color=\"green\", annotation_text=f\"Threshold: {threshold_bodytail:,.0f}\", annotation_position=\"top right\")\n\n\n                fig_bodytail.update_layout(title_text=\"Body-Tail ILD Distribution Fit\",\n                                           xaxis_title=\"Loss Amount\", yaxis_title=\"Density\",\n                                           font=dict(size=12), height=450, showlegend=True,\n                                           legend=dict(x=0.01, y=0.99))\n                st.plotly_chart(fig_bodytail, use_container_width=True)\n\n                st.markdown(\"\"\"\n                **Understanding the Body-Tail Fit Plot:**\n                *   The grey histogram shows the empirical distribution of all generated ILD.\n                *   The purple line represents the fitted GPD for losses exceeding the set threshold (green dashed line).\n                    This visual helps confirm how well the GPD captures the extreme events in the tail.\n                \"\"\")\n        else:\n            st.error(\"Could not fit ILD distribution with the provided parameters. Adjust threshold or generate more data.\")\n    else:\n        st.info(\"Generate ILD data first to enable distribution fitting.\")\n```\n\n**application_pages/page3.py**\n```python\nimport streamlit as st\nimport numpy as np\nimport pandas as pd\nimport plotly.graph_objects as go\nfrom scipy.stats import lognorm, norm, genpareto\nfrom plotly.subplots import make_subplots\n\ndef combine_distributions_param_avg(ild_dist_params, scenario_dist_params, n_ild, m_scenario):\n    \"\"\"If both ILD and scenario severities are approximated by Pareto-like tails, this function calculates a combined tail parameter using the weighted average formula.\"\"\"\n    combined_tail_param = (ild_dist_params * n_ild + scenario_dist_params * m_scenario) / (n_ild + m_scenario)\n    return combined_tail_param\n\ndef combine_distributions_quantile_avg_constant_weights(ild_quantile_func, scenario_quantile_func, n_ild, m_scenario, quantiles_to_evaluate):\n    \"\"\"Combines quantile functions using geometric average with constant weights.\"\"\"\n    if not quantiles_to_evaluate:\n        return None\n\n    total_precision = n_ild + m_scenario\n    if total_precision == 0:\n        return [np.nan] * len(quantiles_to_evaluate)\n\n    combined_quantiles = []\n    for q in quantiles_to_evaluate:\n        if n_ild == 0:\n            combined_quantiles.append(scenario_quantile_func(q))\n        elif m_scenario == 0:\n            combined_quantiles.append(ild_quantile_func(q))\n        else:\n            weight_ild = n_ild / total_precision\n            weight_scenario = m_scenario / total_precision\n            combined_quantile = np.power(ild_quantile_func(q), weight_ild) * np.power(scenario_quantile_func(q), weight_scenario)\n            combined_quantiles.append(combined_quantile)\n\n    return combined_quantiles\n\ndef combine_distributions_quantile_avg_variable_weights(ild_quantile_func, scenario_quantile_func, n_ild, m_scenario, quantiles_to_evaluate, num_bootstraps):\n    \"\"\"Combines distributions using quantile averaging with variable weights.\"\"\"\n    combined_quantiles = []\n    for q in quantiles_to_evaluate:\n        # Bootstrapping to estimate variance\n        ild_estimates = [ild_quantile_func(q) for _ in range(num_bootstraps)]\n        scenario_estimates = [scenario_quantile_func(q) for _ in range(num_bootstraps)]\n\n        # Variance calculation\n        ild_var = np.var(ild_estimates) if num_bootstraps > 1 else 1.0\n        scenario_var = np.var(scenario_estimates) if num_bootstraps > 1 else 1.0\n\n        # Weight calculation based on inverse variance\n        ild_weight = 1.0 / (ild_var + 1e-9)  # Adding a small constant to avoid division by zero\n        scenario_weight = 1.0 / (scenario_var + 1e-9)\n\n        # Normalize weights\n        total_weight = ild_weight + scenario_weight\n        ild_weight /= total_weight\n        scenario_weight /= total_weight\n\n        # Weighted geometric average of quantiles\n        ild_quantile = ild_quantile_func(q)\n        scenario_quantile = scenario_quantile_func(q)\n\n        if ild_quantile <= 0 or scenario_quantile <= 0:\n            # Fallback to arithmetic mean if quantiles are non-positive, as geometric mean requires positive values\n            combined_quantile = (ild_weight * ild_quantile) + (scenario_weight * scenario_quantile)\n        else:\n            combined_quantile = (ild_quantile**ild_weight) * (scenario_quantile**scenario_weight)\n\n        combined_quantiles.append(combined_quantile)\n    return combined_quantiles\n\ndef calculate_capital(combined_distribution_or_quantiles, confidence_level):\n    \"\"\"Estimates risk capital (e.g., 99.9\\% VaR, Expected Shortfall).\"\"\"\n    if not isinstance(confidence_level, (int, float)):\n        return None\n    if confidence_level <= 0 or confidence_level >= 1:\n        return None\n    if not combined_distribution_or_quantiles:\n        return None\n    if isinstance(combined_distribution_or_quantiles, (int, float)):\n        # This simplified scaling might need re-evaluation for a direct VaR input\n        return combined_distribution_or_quantiles * confidence_level\n    if isinstance(combined_distribution_or_quantiles, list):\n        if not combined_distribution_or_quantiles:\n            return None\n\n        sorted_quantiles = sorted(combined_distribution_or_quantiles)\n        index = int(confidence_level * (len(sorted_quantiles) - 1))\n        return sorted_quantiles[index]\n    return None\n\ndef simulate_stability_paradox(base_scenario_params, modified_body_scenario_params, ild_params):\n    \"\"\"Simulates stability paradox and returns results.\"\"\"\n\n    # Input validation\n    if not all(isinstance(param, dict) for param in [base_scenario_params, modified_body_scenario_params, ild_params]):\n        raise TypeError(\"All parameters must be dictionaries.\")\n\n    def simulate_scenario(params):\n        \"\"\"Simulates a single scenario.\"\"\"\n        frequency_mean = params.get(\"frequency_mean\")\n        severity_50 = params.get(\"severity_50\")\n        severity_90 = params.get(\"severity_90\")\n        severity_99 = params.get(\"severity_99\")\n        precision = params.get(\"precision\", 10)\n\n        if frequency_mean is None or severity_50 is None or severity_90 is None or severity_99 is None:\n            raise ValueError(\"Missing parameters in scenario definition.\")\n\n        if frequency_mean < 0:\n            raise ValueError(\"Frequency cannot be negative.\")\n\n        num_simulations = 10000 # Hardcoded for simulation, consider making it a UI input\n        losses = []\n\n        if frequency_mean > 0:  # only simulate losses if frequency is positive\n            for _ in range(num_simulations):\n                num_losses = np.random.poisson(frequency_mean)\n                for _ in range(num_losses):\n                    # Simple severity simulation (can be improved with more sophisticated distributions)\n                    u = np.random.uniform()\n                    if u < 0.5:\n                        loss = np.random.uniform(0, severity_50)\n                    elif u < 0.9:\n                        loss = np.random.uniform(severity_50, severity_90)\n                    else:\n                        loss = np.random.uniform(severity_90, severity_99)\n\n                    losses.append(loss)\n        return losses\n\n    def simulate_ild(params):\n        \"\"\"Simulates ILD data.\"\"\"\n        frequency_mean = params.get(\"frequency_mean\")\n        frequency_dispersion = params.get(\"frequency_dispersion\") # Not used in current notebook code for ILD simulation, but could be for other dist.\n        severity_distribution = params.get(\"severity_distribution\")\n        severity_params = params.get(\"severity_params\")\n        num_observations = params.get(\"num_observations\")\n        reporting_threshold = params.get(\"reporting_threshold\")\n\n        if frequency_mean is None or frequency_dispersion is None or severity_distribution is None or severity_params is None or num_observations is None or reporting_threshold is None:\n            raise ValueError(\"Missing parameters in ILD definition.\")\n\n        if reporting_threshold < 0:\n            raise ValueError(\"Reporting threshold should not be negative\")\n\n        ild_losses = []\n        for _ in range(num_observations):\n            num_losses = np.random.poisson(frequency_mean)\n            for _ in range(num_losses):\n                if severity_distribution == \"lognorm\":\n                    s = np.random.lognormal(mean=np.log(severity_params[\"mean\"]), sigma=severity_params[\"std\"]/severity_params[\"mean\"])\n                else: # Default to normal\n                    s = np.random.normal(loc=severity_params[\"mean\"], scale=severity_params[\"std\"])\n                if s > reporting_threshold:\n                    ild_losses.append(s)\n        return ild_losses\n\n    # Simulate base and modified scenarios\n    base_losses = simulate_scenario(base_scenario_params)\n    modified_losses = simulate_scenario(modified_body_scenario_params)\n\n    # Simulate ILD\n    ild_losses = simulate_ild(ild_params)\n\n    # Combine losses\n    combined_base_losses = base_losses + ild_losses\n    combined_modified_losses = modified_losses + ild_losses\n\n    # Calculate capital (e.g., VaR 99.5\\%)\n    def calculate_var(losses, confidence_level=0.995):\n          if not losses:\n              return 0  # or some other appropriate value, like the reporting threshold for ILD\n          return np.quantile(losses, confidence_level)\n\n    var_base = calculate_var(combined_base_losses)\n    var_modified = calculate_var(combined_modified_losses)\n\n    return {\n        \"base_var\": var_base,\n        \"modified_var\": var_modified,\n        \"base_losses\": combined_base_losses,\n        \"modified_losses\": combined_modified_losses\n    }\n\n\ndef run_page3():\n    st.header(\"Distribution Combination and Stability Paradox Simulation\")\n    st.markdown(\"\"\"\n    This section explores methods for combining different risk distributions (scenario and ILD) and demonstrates the 'stability paradox'.\n    The stability paradox highlights how seemingly beneficial changes in the body of a risk distribution can paradoxically increase overall risk.\n    \"\"\")\n\n    st.subheader(\"1. Combining Distributions - Parameter Averaging\")\n    st.markdown(\"\"\"\n    This method combines distributions by averaging their parameters, particularly relevant when both ILD and scenario severities are approximated by Pareto-like tails.\n    It uses a weighted average based on the precision (number of losses) of each source.\n    \"\"\")\n\n    col1, col2 = st.columns(2)\n    with col1:\n        ild_tail_param = st.number_input(\"ILD Tail Parameter (ξ1)\", value=0.5, min_value=0.0, help=\"Shape parameter for ILD's Pareto-like tail.\")\n        n_ild = st.number_input(\"ILD Precision (n)\", value=100, min_value=1, help=\"Effective number of losses representing ILD precision.\")\n    with col2:\n        scenario_tail_param = st.number_input(\"Scenario Tail Parameter (ξ2)\", value=0.7, min_value=0.0, help=\"Shape parameter for Scenario's Pareto-like tail.\")\n        m_scenario = st.number_input(\"Scenario Precision (m)\", value=10, min_value=1, help=\"Effective number of losses representing scenario precision.\")\n\n    combined_tail_param = combine_distributions_param_avg(ild_tail_param, scenario_tail_param, n_ild, m_scenario)\n    st.write(f\"**Combined Tail Parameter (ξ1,2):** {combined_tail_param:.4f}\")\n    st.markdown(\"\"\"\n    The combined tail parameter is calculated as a weighted average of the individual tail parameters. The weights are determined by the precision of each data source.\n    A higher precision for one data source will give its tail parameter more influence on the combined parameter.\n    \"\"\")\n\n    st.subheader(\"2. Combining Distributions - Quantile Averaging with Constant Weights\")\n    st.markdown(\"\"\"\n    This method combines the quantiles of individual ILD and scenario distributions using a geometric average with constant weights.\n    \"\"\")\n    quantiles_to_evaluate = st.multiselect(\"Quantiles to Evaluate (Constant Weights)\", [0.90, 0.95, 0.99, 0.995], default=[0.90, 0.95, 0.99], help=\"Select the percentiles for which to calculate combined quantiles.\")\n\n    # Dummy quantile functions for demonstration\n    def ild_quantile_func(q): return lognorm.ppf(q, s=0.7, scale=1000) # Dummy Log-Normal ILD\n    def scenario_quantile_func(q): return lognorm.ppf(q, s=0.8, scale=5000) # Dummy Log-Normal Scenario\n\n    combined_quantiles_constant = combine_distributions_quantile_avg_constant_weights(ild_quantile_func, scenario_quantile_func, n_ild, m_scenario, quantiles_to_evaluate)\n\n    if combined_quantiles_constant:\n        df_quantiles = pd.DataFrame({\n            \"Quantile\": quantiles_to_evaluate,\n            \"ILD Quantile\": [ild_quantile_func(q) for q in quantiles_to_evaluate],\n            \"Scenario Quantile\": [scenario_quantile_func(q) for q in quantiles_to_evaluate],\n            \"Combined Quantile\": combined_quantiles_constant\n        })\n        st.dataframe(df_quantiles)\n\n        # Plot Quantiles\n        fig_quantiles = go.Figure()\n        fig_quantiles.add_trace(go.Scatter(x=df_quantiles[\"Quantile\"], y=df_quantiles[\"ILD Quantile\"], mode='lines+markers', name='ILD Quantile'))\n        fig_quantiles.add_trace(go.Scatter(x=df_quantiles[\"Quantile\"], y=df_quantiles[\"Scenario Quantile\"], mode='lines+markers', name='Scenario Quantile'))\n        fig_quantiles.add_trace(go.Scatter(x=df_quantiles[\"Quantile\"], y=df_quantiles[\"Combined Quantile\"], mode='lines+markers', name='Combined Quantile'))\n        fig_quantiles.update_layout(title=\"Quantile Averaging with Constant Weights\", xaxis_title=\"Quantile\", yaxis_title=\"Loss Amount\", font=dict(size=12))\n        st.plotly_chart(fig_quantiles, use_container_width=True)\n\n    st.subheader(\"3. Simulating the Stability Paradox\")\n    st.markdown(\"\"\"\n    This section demonstrates the 'stability paradox' where seemingly positive changes in the 'body' of a risk distribution can paradoxically lead to an increase in overall tail risk and implied capital.\n    \"\"\")\n\n    with st.expander(\"Base Scenario Parameters\"):\n        base_scenario_params = {\n            \"frequency_mean\": st.number_input(\"Base Frequency\", value=1.0, help=\"Expected number of base scenario losses per year.\", key=\"base_freq\"),\n            \"severity_50\": st.number_input(\"Base 50th Percentile\", value=1_000, help=\"Base 50th percentile loss amount.\", key=\"base_50\"),\n            \"severity_90\": st.number_input(\"Base 90th Percentile\", value=5_000, help=\"Base 90th percentile loss amount.\", key=\"base_90\"),\n            \"severity_99\": st.number_input(\"Base 99th Percentile\", value=10_000, help=\"Base 99th percentile loss amount.\", key=\"base_99\")\n        }\n    with st.expander(\"Modified Body Scenario Parameters\"):\n        modified_body_scenario_params = {\n            \"frequency_mean\": st.number_input(\"Modified Frequency\", value=0.5, help=\"Expected number of modified scenario losses per year.\", key=\"mod_freq\"),\n            \"severity_50\": st.number_input(\"Modified 50th Percentile\", value=1_000, help=\"Modified 50th percentile loss amount.\", key=\"mod_50\"),\n            \"severity_90\": st.number_input(\"Modified 90th Percentile\", value=5_000, help=\"Modified 90th percentile loss amount.\", key=\"mod_90\"),\n            \"severity_99\": st.number_input(\"Modified 99th Percentile\", value=10_000, help=\"Modified 99th percentile loss amount.\", key=\"mod_99\")\n        }\n    with st.expander(\"ILD Parameters for Paradox\"):\n        ild_params_paradox = {\n            \"frequency_mean\": st.number_input(\"ILD Frequency\", value=10.0, help=\"Expected number of ILD losses per year.\", key=\"ild_freq\"),\n            \"frequency_dispersion\": 1.0, # Not used in the notebook\n            \"severity_distribution\": \"lognorm\",\n            \"severity_params\": {\"mean\": 500, \"std\": 200},\n            \"num_observations\": st.number_input(\"ILD Observations\", value=10, help=\"Number of years of ILD data.\", key=\"ild_obs\"),\n            \"reporting_threshold\": st.number_input(\"ILD Reporting Threshold\", value=100, help=\"Minimum ILD loss to record.\", key=\"ild_thresh\")\n        }\n\n    stability_results = simulate_stability_paradox(base_scenario_params, modified_body_scenario_params, ild_params_paradox)\n\n    if stability_results:\n        st.write(f\"**Base Scenario VaR (99.5\\%):** {stability_results['base_var']:.2f}\")\n        st.write(f\"**Modified Scenario VaR (99.5\\%):** {stability_results['modified_var']:.2f}\")\n\n        if stability_results['modified_var'] > stability_results['base_var']:\n            st.warning(\"Stability Paradox Observed: Modified scenario has a higher VaR than the base scenario, even though the body of the distribution improved.\")\n        else:\n            st.success(\"No Stability Paradox Observed: Modified scenario has a lower VaR than the base scenario.\")\n\n        # Plotting\n        fig_paradox = go.Figure()\n\n        # Plotting histograms of losses\n        fig_paradox.add_trace(go.Histogram(x=stability_results['base_losses'], name='Base Scenario Losses', opacity=0.6))\n        fig_paradox.add_trace(go.Histogram(x=stability_results['modified_losses'], name='Modified Scenario Losses', opacity=0.6))\n\n        fig_paradox.update_layout(barmode='overlay', title=\"Stability Paradox Simulation\", xaxis_title=\"Loss Amount\", yaxis_title=\"Frequency\", font=dict(size=12))\n        st.plotly_chart(fig_paradox, use_container_width=True)\n```\n\n### Step 5: Run the Streamlit Application\n\nNavigate to your project's root directory (where `app.py` is located) in your terminal and run:\n```bash\nstreamlit run app.py\n```\nThis command will open the QuLab application in your web browser.\n\n## 3. Understanding Scenario Distribution Fitting\nDuration: 00:15:00\n\nThe \"Scenario Fitting\" page (handled by `application_pages/page1.py`) is designed to convert qualitative expert judgment into a quantifiable statistical distribution. This is a common practice in operational risk where historical data might be scarce for high-impact, low-frequency events.\n\n### Concept: Scenario Analysis\n\nScenario analysis in operational risk involves defining hypothetical events (e.g., a major cyber-attack, a system failure) and assessing their potential financial impact and frequency based on expert opinion. This qualitative input needs to be transformed into a mathematical distribution to be integrated into quantitative risk models. The Log-Normal distribution is frequently used for loss severities due to its positive support and right-skewed nature, which often characterizes financial losses.\n\n### Code Walkthrough: `application_pages/page1.py`\n\nLet's break down the key functions:\n\n#### `run_page1()`\n\nThis function sets up the Streamlit UI for the Scenario Fitting page.\n*   It uses `st.header` and `st.markdown` for titles and descriptions.\n*   `st.columns` is used to arrange input fields side-by-side for frequency and percentile values, enhancing usability.\n*   `st.number_input` widgets allow users to input numerical values for `frequency`, `percentile_50`, `percentile_90`, and `percentile_99`.\n*   A crucial validation step checks if percentiles are strictly increasing, as this is a fundamental requirement for a valid distribution.\n\n#### `fit_scenario_distribution(frequency, percentile_50, percentile_90, percentile_99)`\n\nThis is the core logic for fitting the Log-Normal distribution.\n\n```python\ndef fit_scenario_distribution(frequency, percentile_50, percentile_90, percentile_99):\n    # ... (omitted Streamlit UI elements for brevity)\n    try:\n        # From 50th percentile: ln(percentile_50) = mu_log + sigma_log * norm.ppf(0.5) = mu_log\n        mu_log = np.log(percentile_50)\n\n        # From 90th percentile: ln(percentile_90) = mu_log + sigma_log * norm.ppf(0.9)\n        # sigma_log = (ln(percentile_90) - mu_log) / norm.ppf(0.9)\n        sigma_log = (np.log(percentile_90) - mu_log) / stats.norm.ppf(0.9)\n\n        s_param = sigma_log\n        loc_param = 0 # Assuming losses are positive\n        scale_param = np.exp(mu_log)\n\n        return stats.lognorm(s=s_param, loc=loc_param, scale=scale_param), (s_param, loc_param, scale_param)\n    except Exception as e:\n        st.error(f\"Error fitting distribution: {e}. Please ensure percentiles are valid and increasing.\")\n        return None, None\n```\n\nThe function approximates the parameters of a Log-Normal distribution using two specified percentiles (50th and 90th).\nFor a Log-Normal random variable $X$, $\\ln(X)$ is normally distributed.\nIf $X \\sim \\text{LogNormal}(\\mu, \\sigma)$, then $\\ln(X) \\sim \\text{Normal}(\\mu_{\\text{log}}, \\sigma_{\\text{log}})$.\nHere, `s` corresponds to $\\sigma_{\\text{log}}$, and `scale` corresponds to $e^{\\mu_{\\text{log}}}$. `loc` is typically fixed at 0 for loss distributions.\n\n1.  **50th Percentile (Median):** For a Log-Normal distribution, the median is $e^{\\mu_{\\text{log}}}$. Therefore, $\\mu_{\\text{log}} = \\ln(\\text{percentile\\_50})$.\n2.  **90th Percentile:** We know that the 90th percentile of $\\ln(X)$ is $\\mu_{\\text{log}} + \\sigma_{\\text{log}} \\cdot \\Phi^{-1}(0.9)$, where $\\Phi^{-1}$ is the inverse CDF of the standard normal distribution.\n    So, $\\ln(\\text{percentile\\_90}) = \\mu_{\\text{log}} + \\sigma_{\\text{log}} \\cdot \\text{norm.ppf}(0.9)$.\n    From this, we can derive $\\sigma_{\\text{log}} = \\frac{\\ln(\\text{percentile\\_90}) - \\mu_{\\text{log}}}{\\text{norm.ppf}(0.9)}$.\n\nThese calculated `mu_log` and `sigma_log` directly translate to the `scale_param` and `s_param` for `scipy.stats.lognorm`.\n\n### Visualizations\n\nThe application plots the Probability Density Function (PDF) and Cumulative Distribution Function (CDF) of the fitted Log-Normal distribution using Plotly.\n*   **PDF:** Shows the shape of the distribution, indicating where loss amounts are most likely to fall.\n*   **CDF:** Illustrates the probability that a loss will be less than or equal to a given amount. You can visually verify if your 50th, 90th, and 99th percentiles align with the CDF curve.\n\n### Practical Exercise\n\n1.  Navigate to the \"Scenario Fitting\" page in QuLab.\n2.  Experiment with different values for the 50th, 90th, and 99th percentiles.\n3.  Observe how the fitted Log-Normal distribution's parameters (s, scale) and its PDF/CDF plots change.\n4.  Try setting percentile values that violate the increasing order (e.g., 90th < 50th) and observe the error message.\n\n<aside class=\"positive\">\nThis page provides a powerful way to translate qualitative expert opinions into quantifiable inputs for your risk models, making scenario analysis a tangible component of your risk framework.\n</aside>\n\n## 4. Generating and Fitting Internal Loss Data (ILD)\nDuration: 00:20:00\n\nThe \"ILD Generation and Fitting\" page (handled by `application_pages/page2.py`) focuses on Internal Loss Data, which is the historical record of an organization's actual operational losses. Since real-world ILD is often sensitive, this page provides a mechanism to generate synthetic (simulated) ILD based on specified frequency and severity distributions, and then to fit statistical distributions to this data.\n\n### Concept: Internal Loss Data (ILD)\n\nILD is a cornerstone of operational risk quantification. It provides an empirical basis for understanding an organization's past loss experience. Actuarial modeling for operational risk typically separates losses into two components:\n1.  **Frequency:** How often loss events occur (e.g., number of events per year). Often modeled with Poisson distribution.\n2.  **Severity:** The financial impact of each loss event. Often modeled with heavy-tailed distributions like Log-Normal, Pareto, or Generalized Pareto Distribution (GPD).\n\nA common challenge with ILD is that small losses are frequent but might be below a reporting threshold, while large losses are rare but highly impactful. This often leads to a \"body\" (small to medium losses) and a \"tail\" (large, extreme losses) in the loss distribution, which might be best modeled separately.\n\n### Code Walkthrough: `application_pages/page2.py`\n\n#### `generate_synthetic_ild()`\n\nThis function simulates operational losses based on user-defined frequency and severity parameters.\n```python\n@st.cache_data\ndef generate_synthetic_ild(frequency_params, severity_params, num_observations, reporting_threshold):\n    losses = []\n    for i in range(num_observations):\n        # Generate number of losses for the year using Poisson\n        num_losses = poisson.rvs(frequency_params['lambda'])\n\n        # Generate severity amounts based on chosen distribution\n        if severity_params['distribution'] == 'lognorm':\n            severity_amounts = lognorm.rvs(s=severity_params['sigma'], loc=0, scale=np.exp(severity_params['mean']), size=num_losses)\n        elif severity_params['distribution'] == 'pareto':\n            severity_amounts = pareto.rvs(severity_params['shape'], loc=severity_params['loc'], scale=severity_params['scale'], size=num_losses)\n        elif severity_params['distribution'] == 'gpd':\n            severity_amounts = genpareto.rvs(severity_params['shape'], loc=severity_params['loc'], scale=severity_params['scale'], size=num_losses)\n        losses.extend(severity_amounts)\n\n    # Filter losses by reporting threshold\n    losses = [loss for loss in losses if loss >= reporting_threshold]\n    # ... (DataFrame creation)\n    return df\n```\n*   `st.cache_data` is used to cache the results of this function, so it doesn't re-run expensive simulations unless inputs change.\n*   **Frequency:** Simulated using a Poisson distribution, characterized by `lambda` (average number of events).\n*   **Severity:** Simulated based on the chosen distribution (Log-Normal, Pareto, or GPD) with their respective parameters.\n*   **Reporting Threshold:** Losses below this value are excluded, mimicking real-world data collection practices where small losses are often not recorded.\n\n#### `fit_ild_distribution()`\n\nThis function fits statistical distributions to the generated ILD. It offers two common approaches:\n1.  **Log-Normal Fit:** A single Log-Normal distribution is fitted to all positive loss amounts using `scipy.stats.lognorm.fit()`. This is a simpler approach, assuming a single distribution can capture the entire range of losses.\n2.  **Body-Tail Fit:** This more sophisticated approach acknowledges that the \"body\" (frequent, smaller losses) and \"tail\" (rare, large losses) of a loss distribution may behave differently.\n    *   **Body:** Represented empirically (via a histogram) for losses below a specified `threshold`.\n    *   **Tail:** Modeled using the Generalized Pareto Distribution (GPD) for losses *exceeding* the `threshold`. The GPD is particularly well-suited for modeling extreme values (tail events). The fitting is done using `scipy.stats.genpareto.fit()`.\n\n**Generalized Pareto Distribution (GPD) Formula:**\nThe cumulative distribution function (CDF) for the GPD is given by:\n$$G_{\\xi,\\sigma}(x) = 1 - \\left(1 + \\frac{\\xi (x-u)}{\\sigma}\\right)^{-1/\\xi}$$\nfor $x > u$, where $\\xi$ is the shape parameter ($c$ in `scipy`), $\\sigma$ is the scale parameter (`scale`), and $u$ is the location parameter (`loc`), which is typically the threshold.\nIf $\\xi = 0$, the GPD simplifies to an exponential distribution:\n$$G_{0,\\sigma}(x) = 1 - e^{-(x-u)/\\sigma}$$\n\n### Visualizations\n\n*   **ILD Histogram:** Displays the distribution of generated losses, allowing visual inspection of the data's shape.\n*   **Fitted Log-Normal Plots:** Shows the PDF and CDF of the fitted Log-Normal distribution compared against the empirical histogram and ECDF of the ILD data.\n*   **Body-Tail Fit Plot:** Visualizes the histogram of all ILD data and overlays the fitted GPD for losses above the threshold, illustrating how the GPD captures the extreme tail behavior.\n\n### Practical Exercise\n\n1.  Navigate to the \"ILD Generation and Fitting\" page.\n2.  **Generate Synthetic ILD:**\n    *   Experiment with different `frequency_lambda_ild` values (e.g., 50, 200).\n    *   Change the `ILD Severity Distribution` to \"pareto\" or \"gpd\" and adjust their parameters. Observe how the shape of the generated ILD histogram changes.\n    *   Adjust the `ILD Reporting Threshold` and note how it affects the number of generated losses.\n3.  **Fit Distribution to ILD:**\n    *   Select \"lognorm\" as `ILD Distribution Fit Type`. Observe the fitted parameters and plots.\n    *   Switch to \"bodytail\". Adjust the `Threshold for Body-Tail Fit`. Try setting it low (e.g., 500) or high (e.g., 50000) and observe the warnings or the GPD fit on the plot. A good threshold usually captures a reasonable number of tail observations (e.g., 5-10% of total data).\n\n<aside class=\"negative\">\nWhen fitting distributions, especially GPD, it's crucial to have sufficient data points in the tail (above the threshold). Too few points can lead to unstable or unreliable parameter estimates. The application includes warnings for this.\n</aside>\n\n## 5. Combining Distributions and The Stability Paradox\nDuration: 00:25:00\n\nThe \"Distribution Combination and Paradox Simulation\" page (handled by `application_pages/page3.py`) is where ILD and scenario data are brought together to form a holistic view of operational risk. This page explores different aggregation techniques and crucially demonstrates the \"stability paradox.\"\n\n### Concept: Combining Distributions\n\nOperational risk capital models often combine internal loss data (ILD), external loss data (ELD), scenario analysis (SA), and business environment and internal control factors (BEICFs). For ILD and scenario data, combining their loss distributions is key to getting a comprehensive view of aggregate risk.\n\n#### 1. Parameter Averaging\n\nThis method is suitable when the distributions being combined (e.g., ILD severity and scenario severity) belong to the same family and are characterized by similar parameters, particularly for their tails (like Pareto or GPD). The idea is to take a weighted average of these parameters, with weights reflecting the *precision* or *confidence* in each data source.\n\n*   **Formula:** The combined tail parameter $\\xi_{1,2}$ is calculated as:\n    $$\\xi_{1,2} = \\frac{\\xi_1 n + \\xi_2 m}{n + m}$$\n    where $\\xi_1$ and $\\xi_2$ are the tail parameters of ILD and scenario respectively, and $n$ and $m$ are their precisions (effective number of losses or degrees of freedom).\n\n#### 2. Quantile Averaging with Constant Weights\n\nThis method combines distributions by averaging their quantiles (e.g., the 99th percentile loss). It's often preferred when the functional forms of the underlying distributions might differ significantly or when direct parameter averaging is not appropriate. Geometric averaging is commonly used, especially for positively skewed loss distributions.\n\n*   **Formula:** The combined quantile $Q_{1,2}(p)$ at probability $p$ is calculated as a weighted geometric mean:\n    $$Q_{1,2}(p) = Q_1(p)^w \\cdot Q_2(p)^{(1-w)}$$\n    where $Q_1(p)$ and $Q_2(p)$ are the $p$-th quantiles of the individual distributions, and $w$ and $(1-w)$ are constant weights (e.g., based on precision or expert judgment). In QuLab, $w = \\frac{n}{n+m}$.\n\n<aside class=\"positive\">\nQuantile averaging is a robust method because it operates on the outputs (quantiles) rather than the underlying distribution parameters, making it more flexible when combining diverse data sources.\n</aside>\n\n### Concept: The Stability Paradox\n\nThe stability paradox is a critical concept in operational risk. It describes a counter-intuitive situation where efforts to improve the `body` (i.e., reduce the frequency or severity of smaller, more common losses) of an operational risk distribution can paradoxically lead to an *increase* in the overall tail risk (e.g., Value-at-Risk at a high confidence level like 99.5% or 99.9%).\n\n**Why does this happen?**\nWhen you improve the body of a distribution, you reduce the probability mass in that region. To maintain the total probability (which must sum to 1), this \"removed\" probability mass must redistribute. If the tail of the distribution remains fixed (or is assumed to be fixed by the scenario input), this mass effectively shifts to the *extreme tail*, making rare, large losses proportionally more likely.\n\nConsider this simplified flow:\n```mermaid\ngraph TD\n    A[Initial Loss Distribution] --> B{Improve Body: Reduce Freq/Severity of Small Losses}\n    B --> C[Reduced Probability Mass in Body]\n    C --> D{Total Probability must remain 1}\n    D --> E[Redistribution of Probability Mass]\n    E --> F{If Tail is Unchanged/Fixed by Scenario}\n    F --> G[Increased Relative Probability Mass in Extreme Tail]\n    G --> H[Higher VaR / Capital Requirement]\n```\n\n### Code Walkthrough: `application_pages/page3.py`\n\n#### `combine_distributions_param_avg()`\n\nThis function implements the parameter averaging discussed above.\n```python\ndef combine_distributions_param_avg(ild_dist_params, scenario_dist_params, n_ild, m_scenario):\n    combined_tail_param = (ild_dist_params * n_ild + scenario_dist_params * m_scenario) / (n_ild + m_scenario)\n    return combined_tail_param\n```\nIt takes individual tail parameters and their respective precisions to compute a weighted average.\n\n#### `combine_distributions_quantile_avg_constant_weights()`\n\nThis function calculates combined quantiles using a geometric average with constant weights.\n```python\ndef combine_distributions_quantile_avg_constant_weights(ild_quantile_func, scenario_quantile_func, n_ild, m_scenario, quantiles_to_evaluate):\n    # ... (omitted checks)\n    combined_quantiles = []\n    for q in quantiles_to_evaluate:\n        weight_ild = n_ild / total_precision\n        weight_scenario = m_scenario / total_precision\n        combined_quantile = np.power(ild_quantile_func(q), weight_ild) * np.power(scenario_quantile_func(q), weight_scenario)\n        combined_quantiles.append(combined_quantile)\n    return combined_quantiles\n```\nIt requires quantile functions for ILD and scenarios (which are dummy Log-Normal functions in this application for demonstration purposes).\n\n#### `simulate_stability_paradox()`\n\nThis is the core function for demonstrating the paradox.\n```python\ndef simulate_stability_paradox(base_scenario_params, modified_body_scenario_params, ild_params):\n    # Simulate losses for base scenario, modified scenario, and ILD\n    base_losses = simulate_scenario(base_scenario_params)\n    modified_losses = simulate_scenario(modified_body_scenario_params)\n    ild_losses = simulate_ild(ild_params)\n\n    # Combine losses (simple addition for demonstration)\n    combined_base_losses = base_losses + ild_losses\n    combined_modified_losses = modified_losses + ild_losses\n\n    # Calculate VaR (99.5%) for combined losses\n    def calculate_var(losses, confidence_level=0.995):\n        if not losses: return 0\n        return np.quantile(losses, confidence_level)\n\n    var_base = calculate_var(combined_base_losses)\n    var_modified = calculate_var(combined_modified_losses)\n\n    return {\n        \"base_var\": var_base,\n        \"modified_var\": var_modified,\n        \"base_losses\": combined_base_losses,\n        \"modified_losses\": combined_modified_losses\n    }\n```\n*   It takes parameters for a `base_scenario`, a `modified_body_scenario` (where the body of the scenario distribution is improved, e.g., lower frequency), and `ild_params`.\n*   The `simulate_scenario` and `simulate_ild` functions within `simulate_stability_paradox` are simplified simulators to quickly generate sample losses for the paradox demonstration.\n*   It combines the simulated losses (by simply concatenating the lists) and then calculates the Value-at-Risk (VaR) at a high confidence level (99.5%).\n*   By comparing `var_base` and `var_modified`, the paradox can be observed.\n\n### Practical Exercise\n\n1.  Navigate to the \"Distribution Combination and Paradox Simulation\" page.\n2.  **Parameter Averaging:** Experiment with different `ILD Tail Parameter` and `Scenario Tail Parameter` values, along with their `Precision` inputs. Observe how the `Combined Tail Parameter` changes.\n3.  **Quantile Averaging:** Select various `Quantiles to Evaluate`. Observe how the `Combined Quantile` changes relative to individual ILD and Scenario Quantiles, both in the table and on the plot.\n4.  **Stability Paradox Simulation:**\n    *   Examine the default parameters. The `Base Scenario Frequency` is 1.0, and `Modified Frequency` is 0.5 (an improvement in the body).\n    *   Click on the expanders to see and modify the parameters.\n    *   Observe the calculated `Base Scenario VaR` and `Modified Scenario VaR`.\n    *   If `Modified Scenario VaR` is higher, the paradox is observed. You can try to exaggerate the effect by significantly reducing the `Modified Frequency` while keeping percentile values similar, or by increasing the `ILD Frequency`.\n    *   The histogram provides a visual cue. The modified scenario's histogram might appear \"smaller\" in the body, but its combined tail might be effectively thicker relative to the base, leading to a higher VaR.\n\n<aside class=\"negative\">\nThe stability paradox highlights a significant challenge in operational risk modeling: improving one aspect of risk can have unexpected consequences for overall capital requirements if not properly accounted for. It emphasizes the importance of holistic risk aggregation.\n</aside>\n\n## 6. Extending QuLab\nDuration: 00:05:00\n\nQuLab provides a solid foundation for understanding key operational risk concepts. Here are several ideas to extend its functionalities and make it even more comprehensive:\n\n1.  **More Distribution Types:**\n    *   **Frequency:** Add Negative Binomial, Binomial, or other discrete distributions for frequency modeling.\n    *   **Severity:** Incorporate more continuous distributions like Weibull, Gamma, or Extreme Value Distributions (EVDs) beyond GPD for tail modeling.\n2.  **Advanced Fitting Techniques:**\n    *   Implement maximum likelihood estimation (MLE) or method of moments (MoM) for distribution fitting, especially for complex distributions.\n    *   Add goodness-of-fit tests (e.g., Kolmogorov-Smirnov, Anderson-Darling) to evaluate how well a chosen distribution fits the data.\n3.  **Sophisticated Aggregation Methods:**\n    *   **Convolution:** Implement numerical convolution (e.g., using Fast Fourier Transform) or Monte Carlo simulation to directly calculate the aggregate loss distribution (sum of frequency and severity distributions), which is the gold standard for operational risk capital.\n    *   Explore copula-based approaches to model dependencies between different loss categories or risk types.\n4.  **External Loss Data (ELD) Integration:**\n    *   Add a section for incorporating ELD, perhaps allowing users to define ELD characteristics and then combine it with ILD and scenarios. This would involve techniques like scaling and blending.\n5.  **User Data Persistence:**\n    *   Implement a way to save and load user-defined parameters or generated data, e.g., using Streamlit's `st.session_state`, or integrating with a simple database.\n6.  **VaR and ES Calculations:**\n    *   Extend the capital calculation to include Expected Shortfall (ES) alongside VaR.\n    *   Allow users to choose different confidence levels for VaR/ES.\n7.  **More Complex Paradox Scenarios:**\n    *   Design specific scenarios to illustrate different facets of the stability paradox, such as changes in body severity vs. frequency.\n8.  **Interactive Visualizations:**\n    *   Add interactive sliders on plots to explore the effect of parameter changes in real-time on distributions or aggregated curves.\n9.  **Deployment:**\n    *   Deploy the Streamlit application to a cloud platform (e.g., Streamlit Community Cloud, Heroku, AWS) to make it accessible to a wider audience.\n\n<aside class=\"positive\">\nBy experimenting with these extensions, you can deepen your understanding of operational risk modeling and enhance your Streamlit development skills simultaneously. This application serves as a launching pad for further exploration in quantitative finance.\n</aside>